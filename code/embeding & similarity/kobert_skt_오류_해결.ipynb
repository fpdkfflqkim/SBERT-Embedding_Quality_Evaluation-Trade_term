{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"1DAkwPJsXbjI"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Uvk-rI88FGeK"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n","  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to c:\\users\\user\\appdata\\local\\temp\\pip-req-build-h2dba6y8\n"]},{"name":"stderr","output_type":"stream","text":["  ERROR: Error [WinError 2] 지정된 파일을 찾을 수 없습니다 while executing command git version\n","ERROR: Cannot find command 'git' - do you have 'git' installed and in your PATH?\n"]}],"source":["!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"OjLP-L1uIu0l"},"outputs":[{"name":"stderr","output_type":"stream","text":["ERROR: Invalid requirement: \"'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer\"\n","Hint: = is not a valid operator. Did you mean == ?\n","'subdirectory'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n","��ġ ������ �ƴմϴ�.\n"]}],"source":["#### kobert 다운로드\n","!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"JQE0tizLXDiT"},"outputs":[],"source":["import pandas as pd\n","import torch\n","from transformers import BertTokenizer, BertModel\n","import tqdm\n","import time\n","import numpy as np"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"nyGhA93AMcE1"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'kobert_tokenizer'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkobert_tokenizer\u001b[39;00m \u001b[39mimport\u001b[39;00m KoBERTTokenizer\n\u001b[0;32m      2\u001b[0m tokenizer \u001b[39m=\u001b[39m KoBERTTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mskt/kobert-base-v1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m tokenizer\u001b[39m.\u001b[39mencode(\u001b[39m\"\u001b[39m\u001b[39m한국어 모델을 공유합니다.\u001b[39m\u001b[39m\"\u001b[39m)\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'kobert_tokenizer'"]}],"source":["from kobert_tokenizer import KoBERTTokenizer\n","tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n","tokenizer.encode(\"한국어 모델을 공유합니다.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LSBjqIy8MpAD"},"outputs":[],"source":["import torch\n","from transformers import BertModel\n","model = BertModel.from_pretrained('skt/kobert-base-v1',output_hidden_states = True)\n","text = \"한국어 모델을 공유합니다.\"\n","inputs = tokenizer.batch_encode_plus([text])\n","out = model(input_ids = torch.tensor(inputs['input_ids']),\n","              attention_mask = torch.tensor(inputs['attention_mask']))\n","out.pooler_output.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y39YIRv1FGbe"},"outputs":[],"source":["# from kobert import get_tokenizer\n","# from kobert import get_pytorch_kobert_model\n","\n","# model, vocab  = get_pytorch_kobert_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"noc8-rFcFB8X"},"outputs":[],"source":["# from kobert_tokenizer import KoBERTTokenizer\n","# tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ou_FCQ96_qZE"},"outputs":[],"source":["# from transformers import BertModel\n","# model = BertModel.from_pretrained('skt/kobert-base-v1')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Euy0e5YvJT59"},"outputs":[],"source":["# from kobert_tokenizer import KoBERTTokenizer\n","\n","# MODEL_NAME = 'skt/kobert-base-v1'\n","# tokenizer = KoBERTTokenizer.from_pretrained(MODEL_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"400EBKUcJWgl"},"outputs":[],"source":["# ###KoBERT모델 불러오기\n","# from transformers import BertModel\n","\n","# model = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JzTnpizm_qWf"},"outputs":[],"source":["model.to('cuda')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GRXvtx3J_qTz"},"outputs":[],"source":["max_len = 512"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RrKZ4nGg_qRO"},"outputs":[],"source":["def tokenizer_(text, max_len, tokenizer):\n","    encoded_dict = tokenizer.encode_plus(text = text,\n","                                     add_special_tokens = True,\n","                                     max_length = max_len,\n","                                     padding = 'max_length',\n","                                     return_attention_mask = True,\n","                                     truncation = True)\n","    \n","    input_id = encoded_dict['input_ids']\n","    token_type_id = encoded_dict['token_type_ids']\n","    attention_mask = encoded_dict['attention_mask']\n","    \n","    return input_id, token_type_id, attention_mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jb_vs9Cm_qOw"},"outputs":[],"source":["def bert_embedding(text):\n","    encoded_dict = tokenizer.encode_plus(text = text,\n","                                     add_special_tokens = True,\n","                                     max_length = max_len,\n","                                     padding = 'max_length',\n","                                     return_attention_mask = True,\n","                                     truncation = True)\n","    \n","    tokens_tensor = torch.tensor([encoded_dict['input_ids']]).to('cuda')  \n","    segment_tensors = torch.tensor([encoded_dict['token_type_ids']]).to('cuda')\n","    attention_tensors = torch.tensor([encoded_dict['attention_mask']]).to('cuda')\n","    \n","    model.eval()\n","\n","    with torch.no_grad():\n","        outputs = model(tokens_tensor, attention_tensors, segment_tensors)\n","        \n","    hidden_states = outputs[2]\n","    \n","    token_mean = []\n","\n","    for h in hidden_states[-4:]:\n","        token_mean.append(torch.mean(h[0], dim=0))\n","\n","    last_four_sentence_embedding = sum(token_mean)\n","    \n","    return last_four_sentence_embedding.cpu().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tlju5SMq_qL8"},"outputs":[],"source":["df = pd.read_excel(r\"/content/drive/MyDrive/PythonWorkSpace/이아영_학술대회/추가실험/관세용어사전_길이순.xlsx\", index_col = 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qDZ5UJf6_qJJ"},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YEX7bSE5_qGc"},"outputs":[],"source":["kobert_embed_word = df['단어이름'].apply(lambda x: bert_embedding(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bWVVqX9X_qD-"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IE-6hNH4_qBZ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6PzkwxkO_p-W"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TFnI9ca5_p7s"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hnBB5YV2_p5M"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q5HebkJo_p2x"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R4PbYshS_p0S"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JxUBQv6l_pxl"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UhwQBRX2_pvH"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HyDHPVOC_psM"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7X0z1f___ppf"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tLLOXmlW_pnC"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fDehpqCV_pkd"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0TSP66Mk_piH"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RM76qUrg_pfj"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p02oWAge_pdE"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Ve3603ujvrhv"},"source":["#KoBERT (monologg/kobert)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aPBnTb8jwA0C"},"outputs":[],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OrNs4e2-gliu"},"outputs":[],"source":["# Import generic wrappers\n","from transformers import AutoModel, AutoTokenizer \n","\n","\n","# Define the model repo\n","model_name = \"monologg/kobert\" \n","\n","\n","# Download pytorch model\n","model = AutoModel.from_pretrained(model_name)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","\n","# Transform input tokens \n","inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n","\n","# Model apply\n","outputs = model(**inputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SlTnFLk_glgf"},"outputs":[],"source":["model.to('cuda')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EYBVzAaZgleC"},"outputs":[],"source":["max_len = 512"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9QXEESVbglbz"},"outputs":[],"source":["def tokenizer_(text, max_len, tokenizer):\n","    encoded_dict = tokenizer.encode_plus(text = text,\n","                                     add_special_tokens = True,\n","                                     max_length = max_len,\n","                                     padding = 'max_length',\n","                                     return_attention_mask = True,\n","                                     truncation = True)\n","    \n","    input_id = encoded_dict['input_ids']\n","    token_type_id = encoded_dict['token_type_ids']\n","    attention_mask = encoded_dict['attention_mask']\n","    \n","    return input_id, token_type_id, attention_mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2DnV7bpIglZP"},"outputs":[],"source":["def bert_embedding(text):\n","    encoded_dict = tokenizer.encode_plus(text = text,\n","                                     add_special_tokens = True,\n","                                     max_length = max_len,\n","                                     padding = 'max_length',\n","                                     return_attention_mask = True,\n","                                     truncation = True)\n","    \n","    tokens_tensor = torch.tensor([encoded_dict['input_ids']]).to('cuda')  \n","    segment_tensors = torch.tensor([encoded_dict['token_type_ids']]).to('cuda')\n","    attention_tensors = torch.tensor([encoded_dict['attention_mask']]).to('cuda')\n","    \n","    model.eval()\n","\n","    with torch.no_grad():\n","        outputs = model(tokens_tensor, attention_tensors, segment_tensors)\n","        \n","    hidden_states = outputs[2]\n","    \n","    token_mean = []\n","\n","    for h in hidden_states[-4:]:\n","        token_mean.append(torch.mean(h[0], dim=0))\n","\n","    last_four_sentence_embedding = sum(token_mean)\n","    \n","    return last_four_sentence_embedding.cpu().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RNlYZ9hF0IgP"},"outputs":[],"source":["text = df[\"단어 설명\"][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ISTqLi-O0Idr"},"outputs":[],"source":["tokenizer(\n","    text,\n","    return_token_type_ids=False,\n","    return_attention_mask=False\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZZ7bmh8a0IYR"},"outputs":[],"source":["print(tokenizer.convert_ids_to_tokens([2, 0, 5330, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6016, 0, 0, 0, 0, 7088, 0, 6116, 0, 7043, 5760, 0, 0, 0, 5944, 0, 7043, 0, 0, 0, 7096, 0, 7043, 0, 0, 0, 0, 7043, 0, 0, 0, 0, 0, 7125, 0, 0, 0, 5548, 6555, 0, 0, 7142, 0, 0, 0, 0, 0, 6555, 0, 0, 0, 0, 7043, 5330, 0, 0, 7125, 0, 0, 0, 0, 7142, 7043, 5859, 6555, 0, 0, 0, 7142, 0, 6502, 7096, 7043, 5760, 0, 0, 0, 0, 0, 6629, 0, 6484, 0, 0, 0, 0, 0, 6555, 0, 0, 0, 0, 0, 5655, 7043, 5760, 6555, 0, 6533, 0, 0, 0, 7096, 0, 5330, 0, 0, 0, 0, 5850, 0, 7127, 0, 0, 0, 0, 6706, 0, 0, 0, 0, 0, 0, 0, 0, 5760, 0, 0, 0, 0, 0, 0, 0, 6555, 0, 0, 0, 0, 0, 7788, 0, 3]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SgZLylWHwd_n"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t1O6EIekwW2X"},"outputs":[],"source":["df = pd.read_excel('/content/drive/MyDrive/아영/제주학회/관세용어사전_길이순.xlsx', index_col = 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ro5RXWIewWso"},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LFH8yLo4wrE8"},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UKeRywBBwm06"},"outputs":[],"source":["kobert_embed = df['단어이름'].apply(lambda x: bert_embedding(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KfegF62419wC"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"private_outputs":true,"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}
